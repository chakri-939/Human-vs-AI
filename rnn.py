# -*- coding: utf-8 -*-
"""RNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/187fKpOZ02hAkvna2omkezx1l8YjnKWPW

<a href="https://colab.research.google.com/github/shreyamaurya029/AI-vs-Human-/blob/main/RNNwithfeaturesFinal.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Embedding, LSTM, Dense, Input, Concatenate,Dropout
from keras.models import Model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder,StandardScaler

df = pd.read_csv("/content/drive/MyDrive/Classroom/traindataset.csv")

df.head(2)

df = df.drop(columns=['text_str'])

label_encoder = LabelEncoder()
df['Label'] = label_encoder.fit_transform(df['Label'])

df['Text'].fillna('', inplace=True)
df['Text'] = df['Text'].astype(str)

X_text = df['Text'].values
Y = df['Label'].values

# Tokenize text
maxlen = 100  # Adjust according to your text length
max_words = 10000  # Adjust according to your vocabulary size

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(X_text)
X_text = tokenizer.texts_to_sequences(X_text)

word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

X_text = pad_sequences(X_text, maxlen=maxlen)

X_text

# Preprocess numerical data
numerical_data = df[['Vocab Size', 'Avg Word Length', 'Density', 'active',
                     'passive', 'noun', 'pron', 'verb', 'adj', 'adv', 'det', 'propn', 'part',
                     'intj', 'punct', 'Flesch Reading Ease', 'Gunning Fog Index',
                     'Perplexity', 'Burstness']].values

scaler = StandardScaler()
numerical_data = scaler.fit_transform(numerical_data)

# Concatenate text and numerical data
X_text = np.asarray(X_text)
numerical_data = np.asarray(numerical_data)
Y = np.asarray(Y)
X = np.concatenate((X_text, numerical_data), axis=1)

X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)

X_text_train = X_train[:, :maxlen]
num_data_train = X_train[:, maxlen:]
X_text_val = X_val[:, :maxlen]
num_data_val = X_val[:, maxlen:]

!pip install keras
from keras.utils import to_categorical
Y_train = to_categorical(Y_train, num_classes=2)
Y_val = to_categorical(Y_val, num_classes=2)

from numpy import array
from numpy import asarray
from numpy import zeros
# Load GloVe word embeddings
embeddings_dictionary = {}
embedding_dim = 100  # Adjust according to the dimensions of your GloVe embeddings file

glove_file = open("/content/drive/MyDrive/Colab Notebooks/glove.6B.300d.txt")  # Update the path
for line in glove_file:
    records = line.split()
    word = records[0]
    vector_dimensions = asarray(records[1:], dtype='float32')
    embeddings_dictionary[word] = vector_dimensions
glove_file.close()

vocab_size = len(tokenizer.word_index) + 1  ## total distinct words is the Vocabulary ##
word_index = tokenizer.word_index
num_words = min(max_words,len(word_index)+1)
embed_size = 300
embedding_matrix = zeros((num_words, embed_size)) ## has to be similar to glove dimension ##
for word, index in tokenizer.word_index.items():
    if index >= max_words:
        continue
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector

embedding_matrix

input_text = Input(shape=(maxlen,), dtype='int32')
embedding_layer = Embedding(num_words, embed_size, weights=[embedding_matrix], trainable=False)(input_text)
lstm_layer = LSTM(128)(embedding_layer)

input_numerical = Input(shape=(numerical_data.shape[1],))
merged = Concatenate()([lstm_layer, input_numerical])
dense_layer = Dense(64, activation='relu')(merged)
dense_layer = Dropout(0.5)(dense_layer)
output = Dense(2, activation='softmax')(dense_layer)

model = Model(inputs=[input_text, input_numerical], outputs=output)
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()

model.fit([X_text_train, num_data_train], Y_train, epochs=10, batch_size=32, validation_data=([X_text_val, num_data_val], Y_val))

# Save the model to a specific path
model.save('/content/drive/MyDrive/Colab Notebooks/RNN')

from sklearn.metrics import classification_report, precision_score, recall_score, f1_score

# Evaluate the model on the validation data
loss, accuracy = model.evaluate([X_text_val, num_data_val], Y_val, verbose=0)

# Predict the classes for the validation data
Y_pred = model.predict([X_text_val, num_data_val])
Y_pred_classes = np.argmax(Y_pred, axis=1)
Y_true = np.argmax(Y_val, axis=1)

# Calculate precision, recall, and F1 score
precision = precision_score(Y_true, Y_pred_classes)
recall = recall_score(Y_true, Y_pred_classes)
f1 = f1_score(Y_true, Y_pred_classes)

print(f'Accuracy: {accuracy:.2f}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1:.2f}')

# Generate and print the classification report
report = classification_report(Y_true, Y_pred_classes, target_names=['Human', 'Machine'])
print(report)

from sklearn.preprocessing import StandardScaler
from keras.models import load_model

# Load the test dataset
test_df = pd.read_csv("/content/drive/MyDrive/Classroom/testdataset.csv")

# **Load or create the training dataset**
# Assuming the training data is in a CSV file
train_df = pd.read_csv("/content/drive/MyDrive/Classroom/testdataset.csv")

# Preprocess the test dataset
test_df['Text'].fillna('', inplace=True)
test_df['Text'] = test_df['Text'].astype(str)

feature_columns = ['Vocab Size', 'Avg Word Length', 'Density', 'active',
                   'passive', 'noun', 'pron', 'verb', 'adj', 'adv', 'det', 'propn',
                   'part', 'intj', 'punct', 'Flesch Reading Ease',
                   'Gunning Fog Index', 'Perplexity', 'Burstness']

scaler = StandardScaler()

# Fit the scaler to your training data
scaler.fit(train_df[feature_columns])
test_df[feature_columns] = scaler.transform(test_df[feature_columns])

# Tokenize the text data
maxlen = 100  # or the length you used during training
X_text_test = tokenizer.texts_to_sequences(test_df['Text'])
X_text_test = pad_sequences(X_text_test, maxlen=maxlen)
num_data_test = test_df[feature_columns].values

# Load the trained RNN model
model_path ='/content/drive/MyDrive/Colab Notebooks/RNN'
model = load_model(model_path)

# Evaluate the model on the test data
Y_test = test_df['Label'].values
Y_test = np.eye(2)[Y_test]
loss, accuracy = model.evaluate([X_text_test, num_data_test], Y_test, verbose=0)
print(f'Test Accuracy: {accuracy*100:.2f}%')

# Predict the classes for the test data
Y_pred = model.predict([X_text_test, num_data_test])
Y_pred_classes = np.argmax(Y_pred, axis=1)
Y_true = np.argmax(Y_test, axis=1)

# Generate and print the classification report
print(f'Accuracy: {accuracy:.2f}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1:.2f}')
from sklearn.metrics import classification_report
report = classification_report(Y_true, Y_pred_classes, target_names=['Class 0', 'Class 1'])
print(report)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, classification_report
# Generate and plot the confusion matrix
cm = confusion_matrix(Y_true, Y_pred_classes)
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Human', 'Machine'], yticklabels=['Human', 'Machine'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Compute ROC curve and AUC
fpr, tpr, _ = roc_curve(Y_true, Y_pred[:, 1])
roc_auc = roc_auc_score(Y_true, Y_pred[:, 1])

# Plot the ROC curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()